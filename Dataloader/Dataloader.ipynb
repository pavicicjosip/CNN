{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ce46603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7880bda8",
   "metadata": {},
   "source": [
    "## ImageAugmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db0ee681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAugmentation:\n",
    "    def __init__(self, path):\n",
    "        self.dim = (64, 36)\n",
    "        self.folder_list = glob.glob(path + \"/*\")\n",
    "        self.folders = []\n",
    "        for p in self.folder_list:\n",
    "            self.folders.append(p.split(\"\\\\\")[-1])\n",
    "        \n",
    "        \n",
    "        for folder in self.folders:\n",
    "            self.file_names = []\n",
    "            self.files = glob.glob(path + \"/\" + folder + \"/*\")\n",
    "            for file in self.files:\n",
    "                #resize\n",
    "                self.file_names.append(file.split(\"\\\\\")[-1])\n",
    "                img = cv2.imread(path + \"/\" + folder + \"/\" + self.file_names[-1])\n",
    "                resized = cv2.resize(img, self.dim,interpolation=cv2.INTER_AREA)                \n",
    "                cv2.imwrite(path + \"/\" + folder + \"/\" + self.file_names[-1],resized)\n",
    "                \n",
    "                #resized img reflection\n",
    "                if self.file_names[-1][:2] != \"r_\" and path.split('/')[-1] != 'test':\n",
    "                    rows, cols, dim = resized.shape\n",
    "                    M = np.float32([[-1,  0, cols],\n",
    "                                    [0 ,  1, 0   ],\n",
    "                                    [0 ,  0, 1   ]])\n",
    "                    reflected_img = cv2.warpPerspective(resized,M,(int(cols),int(rows)))\n",
    "                    cv2.imwrite(path + \"/\" + folder + \"/\" + \"r_\"+ self.file_names[-1],reflected_img)\n",
    "                \n",
    "            \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cdeb289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ImageAugmentation at 0x25246f68f40>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ImageAugmentation(\"./data/train\")\n",
    "ImageAugmentation(\"./data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d850c977",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af34e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDataset(Dataset):\n",
    "    def __init__(self, train = False, test = False):\n",
    "        if(train):\n",
    "            self.data_path = \"./data/train/\"\n",
    "        if(test):\n",
    "            self.data_path = \"./data/test/\"\n",
    "        self.folder_list = glob.glob(self.data_path + \"*\")\n",
    "\n",
    "        self.data = []\n",
    "        for folder in self.folder_list:\n",
    "            folder_name = folder.split(\"\\\\\")[-1]\n",
    "            for img_path in glob.glob(self.data_path + folder_name + \"/*.jpg\"):\n",
    "                self.data.append([img_path, folder_name])\n",
    "        self.class_map = {\"Car\": 0, \"Truck\": 1, \"Bicycle\": 2}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        img_path, class_name = self.data[i]\n",
    "        img = cv2.imread(img_path)\n",
    "        class_id = self.class_map[class_name]\n",
    "        img_tensor = torch.tensor(img, dtype=torch.float)\n",
    "        img_tensor = img_tensor.permute(2,0,1)\n",
    "        class_id = torch.tensor([class_id])\n",
    "        return img_tensor, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25bf4eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    batch_size = 4\n",
    "    train_dataset = CDataset(train=True)\n",
    "    data_loader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    test_dataset = CDataset(test=True)\n",
    "    data_loader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    #for imgs, labels in data_loader:\n",
    "        #print(\"Batch of images has shape: \",imgs.shape)\n",
    "        #print(\"Batch of labels has shape: \", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e6c961",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd5ce841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d27159ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\") \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d7b2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nadodo\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.4), \n",
    "            nn.Linear(18432, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.4), \n",
    "            nn.Linear(4096, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(256, 3),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.conv_layer(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "    \n",
    "net = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fd468bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18432"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_input_dims():\n",
    "        batch_data = torch.zeros((1, 3, 64, 36))\n",
    "        \n",
    "        batch_data = net.conv_layer(batch_data)\n",
    "        \n",
    "        return int(np.prod(batch_data.size()))\n",
    "calc_input_dims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5630198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossF = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af58407b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch #0\n",
      "loss: 117.979\n",
      "Starting epoch #1\n",
      "loss: 81.774\n",
      "Starting epoch #2\n",
      "loss: 56.204\n",
      "Starting epoch #3\n",
      "loss: 42.297\n",
      "Starting epoch #4\n",
      "loss: 37.347\n",
      "Starting epoch #5\n",
      "loss: 25.996\n",
      "Starting epoch #6\n",
      "loss: 23.555\n",
      "Starting epoch #7\n",
      "loss: 15.925\n",
      "Starting epoch #8\n",
      "loss: 14.998\n",
      "Starting epoch #9\n",
      "loss: 15.971\n",
      "Starting epoch #10\n",
      "loss: 10.828\n",
      "Starting epoch #11\n",
      "loss: 13.053\n",
      "Starting epoch #12\n",
      "loss: 2.519\n",
      "Starting epoch #13\n",
      "loss: 5.320\n",
      "Starting epoch #14\n",
      "loss: 2.890\n",
      "Starting epoch #15\n",
      "loss: 5.113\n",
      "Starting epoch #16\n",
      "loss: 14.892\n",
      "Starting epoch #17\n",
      "loss: 10.840\n",
      "Starting epoch #18\n",
      "loss: 2.522\n",
      "Starting epoch #19\n",
      "loss: 2.821\n",
      "Starting epoch #20\n",
      "loss: 2.834\n",
      "Starting epoch #21\n",
      "loss: 0.519\n",
      "Starting epoch #22\n",
      "loss: 0.572\n",
      "Starting epoch #23\n",
      "loss: 10.394\n",
      "Starting epoch #24\n",
      "loss: 8.085\n",
      "Starting epoch #25\n",
      "loss: 4.133\n",
      "Starting epoch #26\n",
      "loss: 1.829\n",
      "Starting epoch #27\n",
      "loss: 1.358\n",
      "Starting epoch #28\n",
      "loss: 2.691\n",
      "Starting epoch #29\n",
      "loss: 1.123\n",
      "Starting epoch #30\n",
      "loss: 2.313\n",
      "Starting epoch #31\n",
      "loss: 4.854\n",
      "Starting epoch #32\n",
      "loss: 8.573\n",
      "Starting epoch #33\n",
      "loss: 2.364\n",
      "Starting epoch #34\n",
      "loss: 0.747\n",
      "Starting epoch #35\n",
      "loss: 0.202\n",
      "Starting epoch #36\n",
      "loss: 5.823\n",
      "Starting epoch #37\n",
      "loss: 9.449\n",
      "Starting epoch #38\n",
      "loss: 5.057\n",
      "Starting epoch #39\n",
      "loss: 0.992\n",
      "Starting epoch #40\n",
      "loss: 0.531\n",
      "Starting epoch #41\n",
      "loss: 1.247\n",
      "Starting epoch #42\n",
      "loss: 2.215\n",
      "Starting epoch #43\n",
      "loss: 8.079\n",
      "Starting epoch #44\n",
      "loss: 3.526\n",
      "Starting epoch #45\n",
      "loss: 12.464\n",
      "Starting epoch #46\n",
      "loss: 2.508\n",
      "Starting epoch #47\n",
      "loss: 0.339\n",
      "Starting epoch #48\n",
      "loss: 0.016\n",
      "Starting epoch #49\n",
      "loss: 1.892\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net.to(device)  \n",
    "\n",
    "for epoch in range(50): \n",
    "    print(\"Starting epoch #\" + str(epoch))\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(data_loader_train, 0):\n",
    "        # inputs, labels = data\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = lossF(outputs, labels.flatten())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0 and i != 0: \n",
    "            print('loss: %.3f' %\n",
    "                  (running_loss))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc25e408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv_layer): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "    (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layer): Sequential(\n",
       "    (0): Dropout(p=0.4, inplace=False)\n",
       "    (1): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=256, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Dropout(p=0.4, inplace=False)\n",
       "    (7): Linear(in_features=256, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.to(torch.device(\"cpu\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5c01666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 32\n",
      "Accuracy of the network test images: 88 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in data_loader_test:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            try:\n",
    "                if outputs[i][int(labels[i])] == torch.max(outputs[i]):\n",
    "                    correct += 1\n",
    "            except:\n",
    "                pass\n",
    "            total += 1\n",
    "        \n",
    "    print(correct, total)\n",
    "    \n",
    "print('Accuracy of the network test images: %d %%' % (\n",
    "    round(100 * correct / total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42847f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcfca68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
