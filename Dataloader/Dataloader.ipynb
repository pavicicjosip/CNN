{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ce46603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7880bda8",
   "metadata": {},
   "source": [
    "## ImageAugmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db0ee681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAugmentation:\n",
    "    def __init__(self, path = \"./\"):\n",
    "        self.dim = (64, 36)\n",
    "        self.folder_list = glob.glob(path + \"/*\")\n",
    "        self.folders = []\n",
    "        for p in self.folder_list:\n",
    "            self.folders.append(p.split(\"\\\\\")[-1])\n",
    "            \n",
    "        \n",
    "        \n",
    "        for folder in self.folders:\n",
    "            self.files = glob.glob(path + \"/\" + folder + \"/*\")\n",
    "            \n",
    "            for file in self.files:\n",
    "                #resize\n",
    "                file_name = file.split(\"\\\\\")[-1]\n",
    "                img = cv2.imread(path + \"/\" + folder + \"/\" + file_name)\n",
    "                resized = cv2.resize(img, self.dim,interpolation=cv2.INTER_AREA)                \n",
    "                cv2.imwrite(path + \"/\" + folder + \"/\" + file_name,resized)\n",
    "                \n",
    "                \n",
    "                constraint = [\"rx_\", \"ry_\"]\n",
    "                if file_name[:3] not in constraint and path.split('/')[-1] != 'test':\n",
    "                    rows, cols, dim = resized.shape\n",
    "                    My = np.float32([[-1,  0, cols],\n",
    "                                    [0 ,  1, 0   ],\n",
    "                                    [0 ,  0, 1   ]])\n",
    "                    Mx = np.float32([[1,  0, 0],\n",
    "                                    [0 ,  -1, rows],\n",
    "                                    [0 ,  0, 1   ]])\n",
    "                    #resized img reflection on y axis\n",
    "                    reflected_img = cv2.warpPerspective(resized,My,(int(cols),int(rows)))\n",
    "                    cv2.imwrite(path + \"/\" + folder + \"/\" + \"ry_\"+ file_name,reflected_img)\n",
    "                    \n",
    "                    #resized img reflection on x axis\n",
    "                    reflected_img = cv2.warpPerspective(resized,Mx,(int(cols),int(rows)))\n",
    "                    cv2.imwrite(path + \"/\" + folder + \"/\" + \"rx_\"+ file_name,reflected_img)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cdeb289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ImageAugmentation at 0x1f908d09b20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resize i reflect \n",
    "ImageAugmentation(\"./data/train\")\n",
    "ImageAugmentation(\"./data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d850c977",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af34e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDataset(Dataset):\n",
    "    def __init__(self, train = False, test = False):\n",
    "        if(train):\n",
    "            self.data_path = \"./data/train/\"\n",
    "        if(test):\n",
    "            self.data_path = \"./data/test/\"\n",
    "        self.folder_list = glob.glob(self.data_path + \"*\")\n",
    "\n",
    "        self.data = []\n",
    "        for folder in self.folder_list:\n",
    "            folder_name = folder.split(\"\\\\\")[-1]\n",
    "            for img_path in glob.glob(self.data_path + folder_name + \"/*.jpg\"):\n",
    "                self.data.append([img_path, folder_name])\n",
    "        self.class_map = {\"Car\": 0, \"Truck\": 1, \"Bicycle\": 2, \"Dog\": 3}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        img_path, class_name = self.data[i]\n",
    "        img = cv2.imread(img_path)\n",
    "        class_id = self.class_map[class_name]\n",
    "        img_tensor = torch.tensor(img, dtype=torch.float)\n",
    "        img_tensor = img_tensor.permute(2,0,1)\n",
    "        class_id = torch.tensor([class_id])\n",
    "        return img_tensor, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25bf4eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    batch_size = 5\n",
    "    train_dataset = CDataset(train=True)\n",
    "    data_loader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    test_dataset = CDataset(test=True)\n",
    "    data_loader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    #for imgs, labels in data_loader:\n",
    "        #print(\"Batch of images has shape: \",imgs.shape)\n",
    "        #print(\"Batch of labels has shape: \", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e6c961",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd5ce841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d27159ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\") \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d7b2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.PReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.PReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.PReLU(),\n",
    "            #nadodo\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.PReLU(),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.6), \n",
    "            nn.Linear(18432, 4096),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(p=0.6), \n",
    "            nn.Linear(4096, 256),\n",
    "            nn.PReLU(),\n",
    "            nn.Dropout(p=0.6),\n",
    "            nn.Linear(256, len(train_dataset.class_map)),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.conv_layer(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fd468bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18432"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = CNN()\n",
    "def calc_input_dims():\n",
    "        batch_data = torch.zeros((1, 3, 64, 36))\n",
    "        \n",
    "        batch_data = net.conv_layer(batch_data)\n",
    "        \n",
    "        return int(np.prod(batch_data.size()))\n",
    "calc_input_dims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5630198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossF = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af58407b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch #0\n",
      "loss: 176.911\n",
      "loss: 145.359\n",
      "Starting epoch #1\n",
      "loss: 104.634\n",
      "loss: 118.932\n",
      "Starting epoch #2\n",
      "loss: 92.775\n",
      "loss: 75.690\n",
      "Starting epoch #3\n",
      "loss: 86.183\n",
      "loss: 71.450\n",
      "Starting epoch #4\n",
      "loss: 64.733\n",
      "loss: 57.356\n",
      "Starting epoch #5\n",
      "loss: 46.209\n",
      "loss: 48.775\n",
      "Starting epoch #6\n",
      "loss: 54.409\n",
      "loss: 58.421\n",
      "Starting epoch #7\n",
      "loss: 42.278\n",
      "loss: 49.761\n",
      "Starting epoch #8\n",
      "loss: 29.930\n",
      "loss: 35.085\n",
      "Starting epoch #9\n",
      "loss: 29.661\n",
      "loss: 27.864\n",
      "Starting epoch #10\n",
      "loss: 26.450\n",
      "loss: 34.967\n",
      "Starting epoch #11\n",
      "loss: 28.185\n",
      "loss: 31.674\n",
      "Starting epoch #12\n",
      "loss: 21.269\n",
      "loss: 23.779\n",
      "Starting epoch #13\n",
      "loss: 19.474\n",
      "loss: 24.994\n",
      "Starting epoch #14\n",
      "loss: 23.893\n",
      "loss: 20.869\n",
      "Starting epoch #15\n",
      "loss: 19.600\n",
      "loss: 17.267\n",
      "Starting epoch #16\n",
      "loss: 22.658\n",
      "loss: 15.682\n",
      "Starting epoch #17\n",
      "loss: 15.123\n",
      "loss: 12.748\n",
      "Starting epoch #18\n",
      "loss: 9.914\n",
      "loss: 11.638\n",
      "Starting epoch #19\n",
      "loss: 13.008\n",
      "loss: 13.204\n",
      "Starting epoch #20\n",
      "loss: 8.934\n",
      "loss: 8.208\n",
      "Starting epoch #21\n",
      "loss: 10.494\n",
      "loss: 13.209\n",
      "Starting epoch #22\n",
      "loss: 20.923\n",
      "loss: 7.590\n",
      "Starting epoch #23\n",
      "loss: 15.889\n",
      "loss: 11.174\n",
      "Starting epoch #24\n",
      "loss: 24.158\n",
      "loss: 17.361\n",
      "Starting epoch #25\n",
      "loss: 22.847\n",
      "loss: 6.348\n",
      "Starting epoch #26\n",
      "loss: 14.602\n",
      "loss: 8.773\n",
      "Starting epoch #27\n",
      "loss: 4.192\n",
      "loss: 7.054\n",
      "Starting epoch #28\n",
      "loss: 9.166\n",
      "loss: 4.736\n",
      "Starting epoch #29\n",
      "loss: 9.766\n",
      "loss: 9.623\n",
      "Starting epoch #30\n",
      "loss: 7.504\n",
      "loss: 8.343\n",
      "Starting epoch #31\n",
      "loss: 8.115\n",
      "loss: 11.094\n",
      "Starting epoch #32\n",
      "loss: 10.210\n",
      "loss: 33.147\n",
      "Starting epoch #33\n",
      "loss: 8.941\n",
      "loss: 4.545\n",
      "Starting epoch #34\n",
      "loss: 3.393\n",
      "loss: 10.013\n",
      "Starting epoch #35\n",
      "loss: 2.572\n",
      "loss: 6.601\n",
      "Starting epoch #36\n",
      "loss: 4.549\n",
      "loss: 17.105\n",
      "Starting epoch #37\n",
      "loss: 5.100\n",
      "loss: 12.749\n",
      "Starting epoch #38\n",
      "loss: 11.724\n",
      "loss: 13.050\n",
      "Starting epoch #39\n",
      "loss: 9.887\n",
      "loss: 5.441\n",
      "Starting epoch #40\n",
      "loss: 4.715\n",
      "loss: 3.889\n",
      "Starting epoch #41\n",
      "loss: 2.420\n",
      "loss: 1.098\n",
      "Starting epoch #42\n",
      "loss: 11.231\n",
      "loss: 4.673\n",
      "Starting epoch #43\n",
      "loss: 4.053\n",
      "loss: 1.899\n",
      "Starting epoch #44\n",
      "loss: 9.443\n",
      "loss: 5.697\n",
      "Starting epoch #45\n",
      "loss: 3.431\n",
      "loss: 3.154\n",
      "Starting epoch #46\n",
      "loss: 12.073\n",
      "loss: 13.476\n",
      "Starting epoch #47\n",
      "loss: 12.645\n",
      "loss: 9.593\n",
      "Starting epoch #48\n",
      "loss: 6.158\n",
      "loss: 11.557\n",
      "Starting epoch #49\n",
      "loss: 7.434\n",
      "loss: 2.752\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net.to(device)  \n",
    "\n",
    "for epoch in range(50): \n",
    "    print(\"Starting epoch #\" + str(epoch))\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(data_loader_train, 0):\n",
    "        # inputs, labels = data\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = lossF(outputs, labels.flatten())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0 and i != 0: \n",
    "            print('loss: %.3f' %\n",
    "                  (running_loss))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc25e408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv_layer): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): PReLU(num_parameters=1)\n",
       "    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): PReLU(num_parameters=1)\n",
       "    (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): PReLU(num_parameters=1)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): PReLU(num_parameters=1)\n",
       "    (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): PReLU(num_parameters=1)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layer): Sequential(\n",
       "    (0): Dropout(p=0.6, inplace=False)\n",
       "    (1): Linear(in_features=18432, out_features=4096, bias=True)\n",
       "    (2): PReLU(num_parameters=1)\n",
       "    (3): Dropout(p=0.6, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=256, bias=True)\n",
       "    (5): PReLU(num_parameters=1)\n",
       "    (6): Dropout(p=0.6, inplace=False)\n",
       "    (7): Linear(in_features=256, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.to(torch.device(\"cpu\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5c01666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 40\n",
      "Accuracy on test images: 88 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in data_loader_test:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            try:\n",
    "                if outputs[i][int(labels[i])] == torch.max(outputs[i]):\n",
    "                    correct += 1\n",
    "                total+=1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    print(correct, total)\n",
    "\n",
    "print('Accuracy on test images: %d %%' % (\n",
    "    round(100 * correct / total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42847f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./CNNmodule\"\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcfca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
